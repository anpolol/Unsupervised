{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-gN3S4NhAjs",
    "outputId": "fef06879-cb42-4f09-db2d-88671e8c4315"
   },
   "outputs": [],
   "source": [
    "#!pip install -q torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting virtualenv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script virtualenv.exe is installed in 'C:\\Users\\User\\AppData\\Roaming\\Python\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\user\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading virtualenv-20.10.0-py2.py3-none-any.whl (5.6 MB)\n",
      "Collecting distlib<1,>=0.3.1\n",
      "  Downloading distlib-0.3.3-py2.py3-none-any.whl (496 kB)\n",
      "Collecting platformdirs<3,>=2\n",
      "  Downloading platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six<2,>=1.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from virtualenv) (1.14.0)\n",
      "Collecting filelock<4,>=3.2\n",
      "  Downloading filelock-3.3.2-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from virtualenv) (4.0.0)\n",
      "Collecting backports.entry-points-selectable>=1.0.4\n",
      "  Downloading backports.entry_points_selectable-1.1.0-py2.py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=0.12->virtualenv) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=0.12->virtualenv) (3.5.0)\n",
      "Installing collected packages: platformdirs, filelock, distlib, backports.entry-points-selectable, virtualenv\n",
      "Successfully installed backports.entry-points-selectable-1.1.0 distlib-0.3.3 filelock-3.3.2 platformdirs-2.4.0 virtualenv-20.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --user virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"source\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!source myenv/Scripts/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q --no-index torch-scatter -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-sparse -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-cluster -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install -q --no-index torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.7.0+cpu.html\n",
    "#!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# link prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from torch_geometric.data import NeighborSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "from modules.model import Net\n",
    "from sklearn.metrics import f1_score\n",
    "from modules.sampling import Sampler, SamplerContextMatrix, SamplerRandomWalk,SamplerFactorization,SamplerAPP\n",
    "from datetime import datetime\n",
    "import random\n",
    "from torch_geometric.data import GraphSAINTNodeSampler\n",
    "import pickle\n",
    "from torch_geometric.data import Data\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocess(data):\n",
    "        #splitting data to train and test\n",
    "        train_edge_index = []\n",
    "        test_edge_index = []\n",
    "        val_edge_index = []\n",
    "        d = datetime.now()\n",
    "        indices_to_delete_for_val  = random.choices(list(range(len(data.edge_index[0]))), k = int(len(data.edge_index[0])*0.2))\n",
    "        indices_to_delete_for_test  = random.choices( list(set(range(len(data.edge_index[0]))) - set(indices_to_delete_for_val)) , k = int(len(list(set(range(len(data.edge_index[0]))) - set(indices_to_delete_for_val))) *0.625))\n",
    "      #  print(datetime.now()-d)\n",
    "        d = datetime.now()\n",
    "        for i,x in enumerate(list(zip(*data.edge_index.tolist()))):\n",
    "            if i in indices_to_delete_for_test:\n",
    "                test_edge_index.append(x)\n",
    "            elif i in indices_to_delete_for_val: \n",
    "                val_edge_index.append(x)\n",
    "            else: \n",
    "                train_edge_index.append(x)\n",
    "        val_edge_index = torch.tensor(np.array(list(zip(*val_edge_index))))\n",
    "        test_edge_index = torch.tensor(np.array(list(zip(*test_edge_index))))\n",
    "        train_edge_index = torch.tensor(np.array(list(zip(*train_edge_index))), dtype = torch.long)\n",
    "       # print(datetime.now()-d)\n",
    "        d = datetime.now()\n",
    "        s = set(itertools.combinations(range(len(data.x)), 2))\n",
    "       # print(datetime.now()-d)\n",
    "        d = datetime.now()\n",
    "        s_of_edges = set()\n",
    "        for pair in (data.edge_index.t().tolist()):\n",
    "            s_of_edges.add(tuple(pair))\n",
    "        s_of_non_edges = s - s_of_edges\n",
    "        #append negative samples to test set\n",
    "        non_edges=[]\n",
    "      #  print(datetime.now()-d)\n",
    "        d = datetime.now()\n",
    "        for pair in list(s_of_non_edges):\n",
    "            non_edges.append(list(pair))\n",
    "      #  print(datetime.now()-d)\n",
    "        d = datetime.now()   \n",
    "        non_edges_test=torch.tensor(random.choices(non_edges, k = len(test_edge_index[0]))).t()\n",
    "        non_edges_val=torch.tensor(random.choices(non_edges, k = len(val_edge_index[0]))).t()\n",
    "        y_true_val = [1]*len(val_edge_index[0])\n",
    "        y_true_test = [1]*len(test_edge_index[0])\n",
    "        test_edge_index=torch.cat((test_edge_index,non_edges_test),1)\n",
    "        val_edge_index=torch.cat((val_edge_index,non_edges_val),1)\n",
    "        y_true_test += [0]*len(non_edges_test[0])\n",
    "        y_true_val += [0]*len(non_edges_val[0])\n",
    "        data.edge_index = train_edge_index\n",
    "        return data,non_edges_test,non_edges_val,test_edge_index,val_edge_index,y_true_test,y_true_val\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())[0]\n",
    "data,non_edges_test,non_edges_val,test_edge_index,val_edge_index,y_true_test,y_true_val=dataprocess(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprocess_train(data):\n",
    "        #splitting data to train and test\n",
    "        train_edge_index =data.edge_index\n",
    "        s = set(itertools.combinations(range(len(data.x)), 2))\n",
    "       \n",
    "        s_of_edges = set()\n",
    "        for pair in (data.edge_index.t().tolist()):\n",
    "            s_of_edges.add(tuple(pair))\n",
    "        s_of_non_edges = s - s_of_edges\n",
    "        #append negative samples to test set\n",
    "        non_edges=[]\n",
    "      #  print(datetime.now()-d)\n",
    "        d = datetime.now()\n",
    "        for pair in list(s_of_non_edges):\n",
    "            non_edges.append(list(pair))\n",
    "      #  print(datetime.now()-d)\n",
    "        d = datetime.now()   \n",
    "        non_edges_train=torch.tensor(random.choices(non_edges, k = len(train_edge_index[0]))).t()\n",
    "        y_true_train = [1]*len(train_edge_index[0])+[0]*len(train_edge_index[0])\n",
    "        return data,non_edges_train,y_true_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = False\n",
    "if flag:\n",
    "    with open('datasets/CA-GrQc.txt','r') as f:\n",
    "        data_Arxiv = f.readlines()\n",
    "\n",
    "    new_list=list(map(lambda x: x.split('\\t'),data_Arxiv[4:]))\n",
    "    #print(new_list)\n",
    "    dataedge = list(map(lambda x: [int(x[0]),int(x[1].split('\\n')[0])],new_list))\n",
    "    set2=set(collections.Counter((np.array(dataedge)).transpose()[1]).keys()) | set(collections.Counter((np.array(dataedge)).transpose()[0]).keys())\n",
    "    map_ar={}\n",
    "    for j,i in enumerate(set2):\n",
    "        map_ar[i] = j\n",
    "\n",
    "    dataedge_ar_new = list(map(lambda x: [map_ar[x[0]],map_ar[x[1]]], dataedge))\n",
    "\n",
    "    data_edge_index_Arxiv = torch.tensor(dataedge_ar_new).t()\n",
    "    x  = torch.nn.init.xavier_uniform_(torch.empty(len(set2), 128 ))\n",
    "    data = Data(edge_index=data_edge_index_Arxiv,x = x)\n",
    "    data,non_edges_test,non_edges_val,test_edge_index,val_edge_index,y_true_test,y_true_val = dataprocess(data)\n",
    "    with open('data_arxiv.pickle','wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "    with open('non_edges_test_arxiv.pickle','wb') as f:\n",
    "        pickle.dump(non_edges_test,f)\n",
    "    with open('non_edges_val_arxiv.pickle','wb') as f:\n",
    "        pickle.dump(non_edges_val,f)\n",
    "    with open('test_edge_index_arxiv.pickle','wb') as f:\n",
    "        pickle.dump(test_edge_index,f)\n",
    "    with open('y_true_test_arxiv.pickle','wb') as f:\n",
    "        pickle.dump(y_true_test,f)\n",
    "    with open('y_true_val_arxiv.pickle','wb') as f:\n",
    "        pickle.dump(y_true_val,f)\n",
    "    with open('val_edge_index_arxiv.pickle','wb') as f:\n",
    "        pickle.dump(val_edge_index,f)\n",
    "\n",
    "else:\n",
    "    with open('data_arxiv.pickle','rb') as f:\n",
    "        data=pickle.load(f)\n",
    "    with open('non_edges_test_arxiv.pickle','rb') as f:\n",
    "        non_edges_test=pickle.load(f)\n",
    "    with open('non_edges_val_arxiv.pickle','rb') as f:\n",
    "        non_edges_val=pickle.load(f)\n",
    "    with open('test_edge_index_arxiv.pickle','rb') as f:\n",
    "        test_edge_index=pickle.load(f)\n",
    "    with open('y_true_test_arxiv.pickle','rb') as f:\n",
    "        y_true_test=pickle.load(f)\n",
    "    with open('y_true_val_arxiv.pickle','rb') as f:\n",
    "        y_true_val=pickle.load(f)\n",
    "    with open('val_edge_index_arxiv.pickle','rb') as f:\n",
    "        val_edge_index=pickle.load(f)\n",
    "    with open('non_edges_train.pickle','rb') as f:\n",
    "        non_edges_train=pickle.load(f)\n",
    "    with open('y_true_train.pickle','rb') as f:\n",
    "        y_true_train=pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = Planetoid(root='/tmp/Cora', name='Cora',transform=T.NormalizeFeatures())[0]\n",
    "#data,non_edges_test,non_edges_val,test_edge_index,val_edge_index,y_true_test,y_true_val=dataprocess(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,non_edges_train,y_true_train=dataprocess_train(data)\n",
    "#with open('non_edges_train.pickle','wb') as f:\n",
    "#       pickle.dump(non_edges_train,f)\n",
    "#with open('y_true_train.pickle','wb') as f:\n",
    "#       pickle.dump(y_true_train,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(y_pred, y_true):\n",
    "    loss=0\n",
    "    for i,l in enumerate(y_true):\n",
    "        if l == 1:\n",
    "            loss+= -log(y_pred[i])\n",
    "        else:\n",
    "            loss+= -log(1 - y_pred[i])\n",
    "    return loss\n",
    "from torch_geometric.datasets import Flickr\n",
    "class Main():\n",
    "    def __init__(self,conv, dataset, non_edges_test,non_edges_val,test_edge_index,val_edge_index,y_true_test,y_true_val,non_edges_train,y_true_train, device, loss_function,mode,**kwargs):\n",
    "        data = dataset\n",
    "        self.Conv = conv\n",
    "        self.device = device\n",
    "        self.data=data.to(device)\n",
    "        self.non_edges_test =non_edges_test\n",
    "        self.non_edges_val =non_edges_val\n",
    "        self.y_true_val = y_true_val\n",
    "        self.y_true_test =y_true_test \n",
    "        self.test_edge_index =test_edge_index\n",
    "        self.val_edge_index =val_edge_index\n",
    "        self.loss = loss_function\n",
    "        self.mode = mode\n",
    "        self.mask = torch.tensor([True]*len(data.x))\n",
    "        self.flag = self.loss[\"flag_tosave\"]\n",
    "        self.non_edges_train =non_edges_train\n",
    "        self.y_true_train = y_true_train\n",
    "    \n",
    "        super(Main, self).__init__()\n",
    "    \n",
    "    def sampling(self,Sampler,epoch,nodes,loss):\n",
    "        if (epoch == 0): \n",
    "            if self.flag:  \n",
    "                if \"alpha\" in self.loss: \n",
    "                    name_of_file = \"LP_samples_\"+loss[\"Name\"]+\"_alpha_\"+str(self.loss[\"alpha\"])+\".pickle\"\n",
    "                elif \"alpha\" in self.loss: \n",
    "                    name_of_file = \"LP_samples_\"+loss[\"Name\"]+\"_betta_\"+str(self.loss[\"betta\"])+\".pickle\"\n",
    "                else:\n",
    "                    name_of_file = \"LP_samples_\"+loss[\"Name\"]+\".pickle\"\n",
    "                \n",
    "                if os.path.exists(name_of_file):\n",
    "                    with open(name_of_file,'rb') as f:\n",
    "                        self.samples = pickle.load(f)\n",
    "                else:\n",
    "                    self.samples = Sampler.sample(nodes) \n",
    "                    with open(name_of_file,'wb') as f:\n",
    "                        pickle.dump(self.samples,f)\n",
    "            else:\n",
    "                \n",
    "                self.samples = Sampler.sample(nodes)\n",
    "        return self.samples\n",
    "    def train(self, model,data,optimizer,Sampler,train_loader,dropout,epoch,loss):\n",
    "        model.train()   \n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        if model.mode == 'unsupervised':\n",
    "            if model.conv=='GCN':\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                samples = self.sampling(Sampler,epoch, (list(range(len(data.x)))),loss)\n",
    "                loss = model.loss(out, self.samples)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(device) for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id.to(device)].to(device), adjs)\n",
    "                    samples = self.sampling(Sampler,epoch,n_id[:batch_size],loss)   \n",
    "                   \n",
    "                #print(out.shape, self.samples.shape)\n",
    "                    loss = model.loss(out, self.samples)#pos_batch.to(device), neg_batch.to(device))\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()      \n",
    "        else: \n",
    "            if model.conv=='GCN':\n",
    "                out = model.inference(data.to(device),dp=dropout)\n",
    "                out_d = torch.sigmoid(sum((out[data.edge_index[0]]*out[data.edge_index[1]]).t()))\n",
    "                out_n = torch.sigmoid(sum((out[non_edges_train[0]]*out[non_edges_train[1]]).t()))\n",
    "                y_pred = torch.cat((out_d,out_n))  \n",
    "                \n",
    "                y=self.y_true_train\n",
    "                #y_pred = torch.tensor(y_pred).to('cuda')\n",
    "                #print(out[0],out[1])\n",
    "                #print( torch.dot(out[0],out[1]))\n",
    "                y_true = torch.tensor(y).to('cuda')\n",
    "                ones_true = torch.ones(len(y_true)).to('cuda')\n",
    "                ones_pred=torch.ones(len(y_pred)).to('cuda')\n",
    "                loss_vec = (y_true*torch.log(y_pred)+(ones_true-y_true)*torch.log(ones_pred-y_pred))#.requires_grad_(True)\n",
    "                if sum(torch.isnan(loss_vec))>0:\n",
    "                    while sum(torch.isnan(loss_vec))>0:\n",
    "                        if y_true[(torch.isnan(loss_vec)).nonzero()[0]]==1:\n",
    "                            loss_vec[(torch.isnan(loss_vec)).nonzero()[0]]=0\n",
    "                        else:\n",
    "                            print('BIG PROBLEM')\n",
    "                loss = -sum(loss_vec)\n",
    "                total_loss+=loss\n",
    "            else:\n",
    "                for batch_size, n_id, adjs in train_loader:\n",
    "                    if len(train_loader.sizes) == 1:\n",
    "                        adjs = [adjs]\n",
    "                    adjs = [adj.to(device) for adj in adjs]\n",
    "                    out = model.forward(data.x[n_id.to(device)].to(device), adjs)\n",
    "                    out_d = torch.sigmoid(sum((out[data.edge_index[0]]*out[data.edge_index[1]]).t()))\n",
    "                    out_n = torch.sigmoid(sum((out[non_edges_train[0]]*out[non_edges_train[1]]).t()))\n",
    "                    y_pred = torch.cat((out_d,out_n)) \n",
    "                    y=self.y_true_train\n",
    "                    y_true = torch.tensor(y).to('cuda')\n",
    "                    ones_true = torch.ones(len(y_true)).to('cuda')\n",
    "                    ones_pred=torch.ones(len(y_pred)).to('cuda')\n",
    "                    loss_vec = -sum(y_true*torch.log(y_pred)+(ones_true-y_true)*torch.log(ones_pred-y_pred)).requires_grad_()\n",
    "                    total_loss+=loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()      \n",
    "        return total_loss# /len(train_loader)\n",
    "                \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def val_lp(self,model,data,classifier): \n",
    "        model.eval()\n",
    "        out = model.inference(data.to(device))\n",
    "        out = out.cpu()\n",
    "        \n",
    "        y_true = np.array(self.y_true_val)\n",
    "        if True:\n",
    "            y_pred = []\n",
    "            for x in list(zip(*self.val_edge_index)):\n",
    "                y_pred.append(float(torch.sigmoid(torch.dot(out[x[0]],out[x[1]]))))#print(torch.sigmoid(torch.dot(out[x[0]],out[x[1]])))\n",
    "            return roc_auc_score(y_true,np.array(y_pred)) \n",
    "            #return [precision_score(y_true[train_mask.cpu()], best_preds_train, average='macro'), precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]\n",
    "    def test_lp(self,model,data,classifier): \n",
    "        model.eval()\n",
    "        out = model.inference(data.to(device))\n",
    "        out = out.cpu()\n",
    "        y_true = np.array(self.y_true_test)\n",
    "        if True:\n",
    "            y_pred = []\n",
    "            for x in list(zip(*self.test_edge_index)):\n",
    "                y_pred.append(float(torch.sigmoid(torch.dot(out[x[0]],out[x[1]]))))#print(torch.sigmoid(torch.dot(out[x[0]],out[x[1]])))\n",
    "            return roc_auc_score(y_true,torch.tensor(y_pred).cpu().detach().numpy()) \n",
    "      \n",
    "    def run(self,hidden_layer=64,out_layer=128,dropout=0.0,size=1,learning_rate=0.001):\n",
    "        hidden_layer = hidden_layer\n",
    "        out_layer = out_layer\n",
    "        dropout = dropout\n",
    "        size = size\n",
    "\n",
    "        learning_rate = learning_rate\n",
    "    \n",
    "        classifier = \"logistic regression\"\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size = int(len(self.data.x)), sizes=[-1]*size)\n",
    "        Sampler=self.loss[\"Sampler\"]\n",
    "        LossSampler = Sampler('Cora',self.data,device=device,mask=self.mask,loss_info=self.loss)\n",
    "        model = Net(dataset = self.data,mode=self.mode,conv=self.Conv,loss_function=self.loss,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = (size),dropout = dropout)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "                #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, threshold=0.01, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "        scheduler=lr_scheduler.StepLR(optimizer, step_size=25,gamma=0.1)\n",
    "        losses=[]\n",
    "        train_accs=[]\n",
    "        test_accs=[]\n",
    "        val_accs=[]\n",
    "        name_of_plot='conv: '+model.conv+', mode: '+model.mode+', loss from '+self.loss[\"Name\"]\n",
    "\n",
    "        print(name_of_plot)\n",
    "\n",
    "        for epoch in range(100):\n",
    "                    print('epoch',epoch)\n",
    "                    loss = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch,self.loss)\n",
    "                    losses.append(loss.detach().numpy())\n",
    "                    d_test = datetime.now()\n",
    "                    test_acc = self.test_lp(model,self.data,'logistic regression')\n",
    "                    test_accs.append(test_acc)\n",
    "                    log = 'Loss: {:.4f}, Epoch: {:03d}, Test: {:.4f}'\n",
    "                    #scheduler.step()\n",
    "                    print(log.format(loss, epoch, test_acc))\n",
    "        print('Test acc on the last epoch ', test_acc)\n",
    "        plt.plot(losses)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "        plt.plot(test_accs)\n",
    "        plt.title(name_of_plot+' loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()\n",
    "                    #return [precision_score(y_true[train_mask.cpu()], best_preds_train, average='macro'), precision_score(y_true[val_mask.cpu()], best_preds_val, average='macro'),precision_score(y_true[test_mask.cpu()], best_preds, average='macro')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainOptuna(Main):\n",
    "    def objective(self,trial):\n",
    "        # Integer parameter\n",
    "        hidden_layer = trial.suggest_categorical(\"hidden_layer\", [32,64,128,256])\n",
    "        out_layer = trial.suggest_categorical(\"out_layer\", [32,64,128])\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0,0.5,step = 0.1)\n",
    "        size = trial.suggest_categorical(\"size of network, number of convs\", [1,2,3])\n",
    "        Conv = self.Conv# trial.suggest_categorical(\"conv\", [\"SAGE\",\"GCN\",\"GAT\"])\n",
    "\n",
    "        # варьируем параметры\n",
    "        loss_to_train={}\n",
    "        for name in self.loss:\n",
    "            \n",
    "            if type(self.loss[name]) == list :\n",
    "                if len(self.loss[name]) == 3:\n",
    "                    var = trial.suggest_int(name,self.loss[name][0],self.loss[name][1],step=self.loss[name][2])\n",
    "                    loss_to_train[name] = var\n",
    "                elif len(self.loss[name]) == 2:\n",
    "                    var_2 = trial.suggest_float(name,self.loss[name][0],self.loss[name][1])\n",
    "                    loss_to_train[name] = var_2\n",
    "                else:\n",
    "                    var_3 = trial.suggest_categorical(name, self.loss[name])\n",
    "                    loss_to_train[name] = var_3\n",
    "            else:\n",
    "                loss_to_train[name] = self.loss[name]\n",
    "        Sampler =loss_to_train[\"Sampler\"]\n",
    "        model = Net(dataset = self.data,mode=self.mode,conv=Conv,loss_function=loss_to_train,device=device,hidden_layer=hidden_layer,out_layer =out_layer,num_layers = size,dropout = dropout)\n",
    "\n",
    "        train_loader = NeighborSampler(self.data.edge_index, batch_size =int(len(self.data.x)), sizes=[-1]*size)\n",
    "        \n",
    "        #train_loader = GraphSAINTRandomWalkSampler(data, batch_size=2176, walk_length=2,num_steps=5, sample_coverage=100,save_dir=dataset.processed_dir,num_workers=4)\n",
    "\n",
    "        LossSampler = Sampler('Cora',self.data,device=self.device,mask=self.mask,loss_info=loss_to_train)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        learning_rate= trial.suggest_float(\"lr\",5e-4,1e-2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,weight_decay = 1e-5)  \n",
    "\n",
    "        classifier = \"logistic regression\" #trial.suggest_categorical(\"classifier\", [\"logistic regression\", \"catboost\"])\n",
    "\n",
    "        if classifier == \"catboost\":\n",
    "            n_estimators = trial.suggest_int(\"n of estimators\", 10,40,5)\n",
    "            learning_rate_catboost = trial.suggest_float(\"lr_catboost\",5e-4,1e-2)\n",
    "            max_depth = trial.suggest_int(\"max_depth\",1,10,2)\n",
    "        else:\n",
    "            n_estimators = -1\n",
    "            learning_rate_catboost =-1\n",
    "            max_depth = -1\n",
    "        #training of the model\n",
    "        for epoch in range(50):\n",
    "            \n",
    "            loss = self.train(model,self.data,optimizer,LossSampler,train_loader,dropout,epoch,loss_to_train)\n",
    "\n",
    "\n",
    "        val_acc = self.val_lp(model,self.data,classifier)\n",
    "        print(val_acc)\n",
    "\n",
    "        trial.report(val_acc,epoch)\n",
    "\n",
    "        return val_acc\n",
    "\n",
    "    \n",
    "    def run(self,number_of_trials):\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\",study_name=self.loss[\"Name\"]+\" loss,\"+str(self.Conv)+\" conv\")\n",
    "        study.optimize(self.objective,n_trials = number_of_trials)\n",
    "\n",
    "        print('Best trial:')\n",
    "        trial = study.best_trial\n",
    "        print(\" Value: \", trial.value)\n",
    "        print(\" Params: \")\n",
    "        for key, value in trial.params.items():\n",
    "            print(\" {}: {}\".format(key,value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.sampling import Sampler, SamplerContextMatrix, SamplerRandomWalk,SamplerFactorization,SamplerAPP\n",
    "#хорошо\n",
    "DeepWalk = {\"Name\": \"DeepWalk\",\"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":[1,21,5],\"context_size\" : [5,20,5],\"p\":1,\"q\":1,\"loss var\": \"Random Walks\",\"flag_tosave\":False,\"Sampler\" : SamplerRandomWalk } #Проблемы с памятью после того, как увеличила количество тренировочных данных\n",
    "Node2Vec = {\"Name\": \"Node2Vec\",\"walk_length\":[5,20,5],\"walks_per_node\":[5,20,5],\"num_negative_samples\":[1,21,5],\"context_size\" : [5,20,5],\"p\": [0.25, 0.50, 1, 2, 4] ,\"q\":[0.25, 0.50, 1, 2, 4], \"loss var\": \"Random Walks\",\"flag_tosave\":False,\"Sampler\": SamplerRandomWalk}#то же самое \n",
    "LINE = {\"Name\": \"LINE\",\"C\": \"Adj\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix} \n",
    "\n",
    "#плохо\n",
    "HOPE_RPR = {\"Name\": \"HOPE_RPR\",\"C\":\"RPR\",\"loss var\": \"Factorization\",\"flag_tosave\":False,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} #проверить\n",
    "HOPE_Katz = {\"Name\": \"HOPE_Katz\",\"C\":\"Katz\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"betta\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} #проверить\n",
    "HOPE_AA = {\"Name\": \"HOPE_AdamicAdar\",\"C\":\"AA\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} \n",
    "LapEigen = {\"Name\": \"LaplacianEigenMaps\", \"C\":\"Adj\",\"loss var\": \"Laplacian EigenMaps\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]}\n",
    "VERSE_PPR =  {\"Name\": \"VERSE_PPR\",\"C\": \"PPR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerContextMatrix}\n",
    "\n",
    "VERSE_SR =  {\"Name\": \"VERSE_SimRank\",\"C\": \"SR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\":SamplerContextMatrix} \n",
    "\n",
    "Struc2Vec ={} #Implement\n",
    "GraphFactorization = {\"Name\": \"Graph_Factorization\",\"C\":\"Adj\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} \n",
    "HOPE_CN = {\"Name\": \"HOPE_CommonNeighbors\",\"C\":\"CN\",\"loss var\": \"Factorization\",\"flag_tosave\":True,\"Sampler\" :SamplerFactorization,\"lmbda\": [0.0,1.0]} \n",
    "APP ={\"Name\": \"APP\",\"C\": \"PPR\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":True,\"alpha\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\"Sampler\" :SamplerAPP}\n",
    "VERSE_Adj =  {\"Name\": \"VERSE_Adj\",\"C\": \"Adj\",\"num_negative_samples\":[1,21,5],\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-04 13:38:35,226]\u001b[0m A new study created in memory with name: LINE loss,SAGE conv\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d2c9d02af61b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLINE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mMO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMainOptuna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SAGE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_edges_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnon_edges_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_edge_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_edge_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_true_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_true_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnon_edges_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_true_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'unsupervised'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mMO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-59af5743f370>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, number_of_trials)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"maximize\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstudy_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" loss,\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" conv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumber_of_trials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best trial:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m         )\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m                 \u001b[0mprogress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             )\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-59af5743f370>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLossSampler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_to_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-bd3887f840cb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, model, data, optimizer, Sampler, train_loader, dropout, epoch, loss)\u001b[0m\n\u001b[0;32m     71\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#pos_batch.to(device), neg_batch.to(device))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mtotal_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device=torch.device('cpu')\n",
    "loss=LINE\n",
    "MO = MainOptuna('SAGE', data, non_edges_test,non_edges_val,test_edge_index,val_edge_index,y_true_test,y_true_val,non_edges_train,y_true_train, device, loss, mode = 'unsupervised')\n",
    "MO.run(number_of_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv: SAGE, mode: unsupervised, loss from LINE\n",
      "epoch 0\n",
      "Loss: 1.9438, Epoch: 000, Test: 0.6289\n",
      "epoch 1\n",
      "Loss: 1.9947, Epoch: 001, Test: 0.6190\n",
      "epoch 2\n",
      "Loss: 1.9122, Epoch: 002, Test: 0.6042\n",
      "epoch 3\n",
      "Loss: 1.9270, Epoch: 003, Test: 0.4272\n",
      "epoch 4\n",
      "Loss: 1.9332, Epoch: 004, Test: 0.4130\n",
      "epoch 5\n",
      "Loss: 1.9303, Epoch: 005, Test: 0.4074\n",
      "epoch 6\n",
      "Loss: 1.9216, Epoch: 006, Test: 0.4005\n",
      "epoch 7\n",
      "Loss: 1.9113, Epoch: 007, Test: 0.4014\n",
      "epoch 8\n",
      "Loss: 1.9021, Epoch: 008, Test: 0.4024\n",
      "epoch 9\n",
      "Loss: 1.9043, Epoch: 009, Test: 0.3934\n",
      "epoch 10\n",
      "Loss: 1.9015, Epoch: 010, Test: 0.3944\n",
      "epoch 11\n",
      "Loss: 1.8992, Epoch: 011, Test: 0.3999\n",
      "epoch 12\n",
      "Loss: 1.8908, Epoch: 012, Test: 0.4008\n",
      "epoch 13\n",
      "Loss: 1.8863, Epoch: 013, Test: 0.3997\n",
      "epoch 14\n",
      "Loss: 1.8653, Epoch: 014, Test: 0.4005\n",
      "epoch 15\n",
      "Loss: 1.8628, Epoch: 015, Test: 0.4062\n",
      "epoch 16\n",
      "Loss: 1.8484, Epoch: 016, Test: 0.4163\n",
      "epoch 17\n",
      "Loss: 1.8366, Epoch: 017, Test: 0.4250\n",
      "epoch 18\n",
      "Loss: 1.8138, Epoch: 018, Test: 0.4281\n",
      "epoch 19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c52ee8af83e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLINE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mMO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'SAGE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_edges_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnon_edges_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_edge_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_edge_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_true_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_true_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnon_edges_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_true_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'unsupervised'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mMO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00998011516799\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-bd3887f840cb>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, hidden_layer, out_layer, dropout, size, learning_rate)\u001b[0m\n\u001b[0;32m    171\u001b[0m                     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0md_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                     \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_lp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'logistic regression'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m                     \u001b[0mtest_accs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m                     \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Loss: {:.4f}, Epoch: {:03d}, Test: {:.4f}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-bd3887f840cb>\u001b[0m in \u001b[0;36mtest_lp\u001b[1;34m(self, model, data, classifier)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_edge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#print(torch.sigmoid(torch.dot(out[x[0]],out[x[1]])))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device=torch.device('cpu')\n",
    "num_negative_samples= 11\n",
    "alpha= 0.7\n",
    "VERSE_PPR =  {\"Name\": \"VERSE_PPR\",\"C\": \"PPR\",\"num_negative_samples\":num_negative_samples,\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"alpha\":alpha,\"Sampler\" :SamplerContextMatrix}\n",
    "LINE = {\"Name\": \"LINE\",\"C\": \"Adj\",\"num_negative_samples\":20,\"loss var\": \"Context Matrix\",\"flag_tosave\":False,\"Sampler\" :SamplerContextMatrix} \n",
    "\n",
    "loss=LINE\n",
    "MO = Main('SAGE', data, non_edges_test,non_edges_val,test_edge_index,val_edge_index,y_true_test,y_true_val,non_edges_train,y_true_train, device, loss, mode = 'unsupervised')\n",
    "MO.run(hidden_layer=64,out_layer=64,dropout=0.3,size=3,learning_rate=0.00998011516799)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "My__RW_Neighbour.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
